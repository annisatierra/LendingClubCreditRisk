# -*- coding: utf-8 -*-
"""Copy of Final Project_IDX Partners Virtual Internship.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QnTLS-l9svZ9sCSUiftTcph6eGsLJtj_

<font size="6">**Classifiying Credit Risk: A Systematic Approach**</font>

<font size="4">ID/X Partners - Data Scientist</font>

Author : Annisa Sekartierra Mulyanto

<a name="0"></a>
## 0 - Intro
### Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# add other

from google.colab import drive
drive.mount('/content/drive')

"""### Metadata"""

df_var = pd.read_excel('/content/drive/MyDrive/coolyeah/rakamin/LCDataDictionary (1).xlsx')
pd.set_option('display.max_rows', None)
pd.set_option('display.max_colwidth', None)
display(df_var)

"""Teridentifikasi bahwa variabel target adalah `loan_status`

### Dataset
"""

df= pd.read_csv('/content/drive/MyDrive/coolyeah/rakamin/loan_data_2007_2014.csv')
pd.set_option('display.max_colwidth', None)
display(df)

df.info()

"""<a name="1"></a>
## 1 - Data Understanding

Variabel target pada data yang digunakan adalah `loan_status` yang menunjukkan status peminjaman saat ini. Pada kolom ini terdapat 7 kategori dengan penjelasan sebagai berikut:

- **Current**: Pinjaman up-to-date pada semua pembayaran yang belum dilunasi.
- **In Grace Period**: Pinjaman sudah jatuh tempo tetapi masih dalam periode tenggang 15 hari.
- **Late (16-30)**: Pinjaman tidak up-to-date selama 16 hingga 30 hari.
- **Late (31-120)**: Pinjaman tidak up-to-date selama 31 hingga 120 hari.
- **Fully paid**: Pinjaman telah dilunasi sepenuhnya, baik pada akhir periode 3 atau 5 tahun atau sebagai hasil dari pelunasan di muka.
- **Default**: Pinjaman tidak up-to-date untuk jangka waktu yang lama.
- **Charged Off**: Pinjaman yang tidak lagi diharapkan untuk menerima pembayaran lebih lanjut.

Kami mengidentifikasikan suatu **pinjaman yang baik** (1) apabila status pinjaman saat ini berada pada kategori ‘Current’, ‘In Grace Period’, dan ‘Fully Paid’. Sedangkan kategori yang lainnya akan diidentifikasikan sebagai **pinjaman yang tidak baik** (0)

<a name="2"></a>
## 2 - Preprocessing Data (1.1)
Dilakukan data cleaning dengan tujuan memperbaiki atau menghapus kesalahan, ketidakkonsistenan, dan ketidakakuratan dalam kumpulan data.

---
**Cek Duplikasi**
"""

df.duplicated().sum()
# tidak ada duplikasi

"""---
**Cek Kolom Dengan seluruh value NaN**
"""

df.describe().T

"""Akan dihapuskan kolom-kolom berisi NaN yang tidak dapat digunakan pada analisis."""

df1 = df.dropna(axis=1, how='all')
df1.info()

"""Akan dihapuskan juga kolom-kolom yang tidak memiliki hubungan langsung dengan tujuan analisis, yaitu kolom unnamed, url, desc, title, dan zip_code. Kolom unnamed berisikan indeks data yang tidak dibutuhkan sebagai variabel. Kolom url, desc, dan title mengandung informasi teks mengenai member yang tidak dapat dilakukan analisis sesuai dengan tujuan. Kolom zip_code berisikan kode pos yang memiliki nilia berbeda untuk setiap member dan memiliki informasi yang sama dengan kolom addr_state sebagai informasi mengenai alamat.

Oleh karena itu, keempat kolom tidak akan diikutkan pada analisis dan dihapuskan pada step berikut.

Akan dihapus pula kolom id dan member_id karena memiliki nilai unik untuk setiap member sehingga tidak membantu dalam analisis kedepannya.
"""

df1 = df1.drop(['Unnamed: 0', 'url', 'desc', 'title', 'zip_code', 'id', 'member_id'], axis = 1)

df1.info()

"""---
**Cek Kolom Object**

Selanjutnya akan dicek nilai-nilai unik dari variabel kategorik untuk melihat apakah terdapat data yang memiliki format berbeda atau tidak masuk akal.
"""

var_cat = df1.select_dtypes(include = 'object').columns #ambil kolom kategorik

# cek unique values dari tiap kolom kategorik
for kolom in var_cat:
    unique = df1[kolom].unique()
    print(f"Kolom : {kolom} \n {unique}\n\n")

"""Ditemukan beberapa kolom yang memuat informasi mengenai tanggal masih terformat sebagai object. Terdapat pula kolom object yang memiliki missing values. Selain itu terdapat suatu kolom yang hanya memiliki satu kategori, yaitu kolom `aplication_type` yang mana akan dihapuskan karena tidak berguna dalam proses analisis.

"""

df1 = df1.drop(['application_type'], axis = 1)

df2 = df1.copy()

# menambahkan tanggal day pada setiap value dan dianggap sebagai awal bulan
# agar dapat diubah format date
date_format = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d']

df2[date_format] = df2[date_format].apply(lambda x: '01-' + x)

df2[date_format].head()

for col in date_format:
    df2[col] = pd.to_datetime(df2[col], format='%d-%b-%y')

df2[date_format].head()

"""Selanjutnya juga akan di cek jumlah nilai unik untuk setiap kolom object."""

object_columns = df2.select_dtypes(include=['object']).columns
df2[object_columns].nunique()

"""Kolom `emp_title` memiliki 200 ribu lebih nilai unik. Dikarena kurangnya pengetahuan mengenai tipe pekerjaan setiap nilai unik tersebut kami tidak dapat melakukan rekategorisasi pekerjaan. Dengan banyaknya kategori ini akan meningkatkan kinerja komputasi dan berisiko mengurangi akurasi model. Oleh karena itu kami rasa akan lebih baik untuk menghilangkan fitur ini dibandingkan melakukan ekstraksi fitur atau menggunakan fitur ini apa adanya.

Selanjutnya akan dihapus kolom `emp_title`
"""

df2 = df2.drop(['emp_title'], axis = 1)

"""selanjutnya pula akan dicek jumlah setiap nilai unik untuk setiap kolom kategorik."""

object_columns

object_columns = df2.select_dtypes(include=['object']).columns

unik_cat = {}
for col in object_columns:
    unik_cat[col] = df2[col].value_counts()

# Menampilkan hasil
for col, counts in unik_cat.items():
    print(f"Jumlah nilai unik untuk kolom '{col}':")
    print(counts)
    print()

"""---
**Cek Variabel Numerik**
"""

df2.describe().T

"""Terlihat disini bahwa variabel policy_code memiliki nilia yang sama untuk seluruh datanya karena memiliki nilia max dan min yang sama dan standar deviasi bernilai nol. Oleh karena itu, akan dihapuskan kolom `policy_code` sebagai berikut."""

df_p1 = df2.drop(['policy_code'], axis =1)

"""

---
**Rekategorisasi Variabel Target**

Akan dilakukan rekategorisasi variabel target `loan_status` menjadi 0 (Tidak Baik) atau 1 (Baik) bedasarkan business understanding."""

df_p1['loan_status'] = df_p1['loan_status'].replace(['Current', 'Fully Paid', 'In Grace Period', 'Does not meet the credit policy. Status:Fully Paid'], 1).replace(['Charged Off', 'Late (31-120 days)', 'Late (16-30 days)', 'Default', 'Does not meet the credit policy. Status:Charged Off'], 0)
df_p1['loan_status'].value_counts()

"""<a name="3"></a>
## 3 - Exploratory Data Analysis (EDA)
Tahapan ini dilakukan untuk mendapatkan insight yang bermanfaat dari data.

#### 3.1 - Bagaimana sebaran proporsi pada label target (`loan_status`)?
"""

valpie = df_p1['loan_status'].value_counts().values
labelpie = df_p1['loan_status'].unique().tolist()
plt.pie(valpie, labels=['Pinjaman Baik (1)', 'Pinjaman Tidak Baik (0)'], autopct='%1.1f%%')
plt.title('Pie Chart Berdasarkan Status Pinjaman')
plt.show()

"""**JAWAB:**<br>
Sebagian besar dari data peminjam memiliki nilai variabel target 'loan_status' 1 atau pinjaman yang baik yaitu sekitar 88,8%. Sisanya memiliki nilai 0 artinya pinjaman tidak baik. Dari sini kami mengetahui bahwa terdapat ketidakseimbangan variabel target.

#### 3.2 - Bagaimana grade peminjam dengan status pinjaman saat ini?
"""

group_counts = df_p1.groupby(['grade', 'loan_status']).size().unstack(fill_value=0)

# Plot stacked bar chart
group_counts.plot(kind='bar', stacked=True)

# Menambahkan label dan judul
plt.xlabel('Category')
plt.ylabel('Count')
plt.title('Frekuensi Grade untuk setiap Loan Status')

group_counts = {}
for category, group in zip(df_p1['grade'], df_p1['loan_status']):
    if category not in group_counts:
        group_counts[category] = [0, 0]
    group_counts[category][group - 1] += 1

# Mengurutkan kategori
sorted_categories = sorted(group_counts.keys())

# Persiapan untuk plot
num_categories = len(sorted_categories)
bar_width = 0.35
index = np.arange(num_categories)

# Plot
fig, ax = plt.subplots()
bar1 = ax.bar(index - bar_width/2, [group_counts[category][0] for category in sorted_categories], bar_width, label='Pinjaman Baik')
bar2 = ax.bar(index + bar_width/2, [group_counts[category][1] for category in sorted_categories], bar_width, label='Pinjaman Tidak Baik')

# Label dan judul
ax.set_xlabel('Category')
ax.set_ylabel('Count')
ax.set_title('Group Bar Chart')
ax.set_xticks(index)
ax.set_xticklabels(sorted_categories)
ax.legend()

# Menampilkan plot
plt.show()

"""**JAWAB:**<br>

Secara keseluruhan kebanyakan peminjam dengan kategori pinjaman baik memiliki grade B sedangkan kebanyakan peminjam dengan kategori pinjaman tidak baik memiliki grade C.

#### 3.3 - Bagaimana korelasi antar fitur numerik?
"""

cor1 = df_p1.corr(numeric_only=True)
plt.figure(figsize=(10,8))
sns.heatmap(cor1,cmap='coolwarm',annot=True,linecolor='white',linewidths=1, fmt = '.2f')

core = df_p1[['loan_amnt','funded_amnt','funded_amnt_inv','installment','total_pymnt','total_pymnt_inv', 'total_rec_prncp']]
cor2 = core.corr(numeric_only=True)
plt.figure(figsize=(10,8))
sns.heatmap(cor2,cmap='coolwarm',annot=True,linecolor='white',linewidths=1, fmt = '.2f')

"""**JAWAB:**<br>
Terdapat korelasi yang kuat antara beberapa fitur. hal ini mungkin akan mengakibatkan multikolinearitas. Oleh karena itu akan dilakukan tahapan pengatasan permasalahan ini salah satunya dengan PCA nantinya.

#### 3.4 - Bagaimana distribusi installment dan loan amount untuk setiap loan status?
"""

s1 = df_p1[df_p1['loan_status'] == 0]['installment']
s2 = df_p1[df_p1['loan_status'] == 1]['installment']

plt.figure(figsize=(10,5))
sns.distplot(s1,  color='blue', label='Pinjaman Tidak Baik')
sns.distplot(s2, color='orange', label='Pinjaman Baik')
plt.title('Distribusi Installment bedasarkan Status Pinjaman')
plt.ylabel('Frequency')
plt.legend()
plt.show()

plt.figure(figsize=(10,5))

sns.boxplot(data=df_p1, x='loan_status', y='loan_amnt', palette = 'pastel')
plt.title('Loan Amount bedasarkan Loan Status')
plt.ylabel('Jumlah')
plt.legend()
plt.show()

"""**JAWAB:**<br>

Tidak terdapat perbedaan yang signifikan diantara kedua status pinjaman, baik untuk nilai Loan Amount atau untuk Installment.

#### 3.5 - Bagaimana perbandingan beberapa variabel kategorik untuk setiap status pinjaman?
"""

plt.figure(figsize=(15, 30))

plt.subplot(6, 2, 1)
sns.countplot(x='term', data=df_p1, hue='loan_status')

plt.subplot(6, 2, 2)
sns.countplot(x='home_ownership', data=df_p1, hue='loan_status')

plt.subplot(6, 2, 3)
sns.countplot(x='verification_status', data=df_p1, hue='loan_status')

plt.subplot(6, 2, 4)
g = sns.countplot(x='emp_length', data=df_p1, hue='loan_status')
g.set_xticklabels(g.get_xticklabels(), rotation=45);

plt.subplot(6, 2, 5)
sns.countplot(x='initial_list_status', data=df_p1, hue='loan_status')

plt.subplot(6, 2, 6)
e = sns.countplot(x='purpose', data=df_p1, hue='loan_status')
e.set_xticklabels(e.get_xticklabels(), rotation=90);

"""<a name="3"></a>
## 4 - Feature Engineering dan Preprocessing Data (1.2)
Akan dilakukan beberapa proses ekstraksi fitur
"""

df4 = df_p1.copy()

df4['month_issue_since_crline'] = (df4['issue_d'].dt.year - df4['earliest_cr_line'].dt.year) * 12 + (df4['issue_d'].dt.month - df4['earliest_cr_line'].dt.month)

df4['month_last_pymnt_since_issue'] = (df4['last_pymnt_d'].dt.year - df4['issue_d'].dt.year) * 12 + (df4['last_pymnt_d'].dt.month - df4['issue_d'].dt.month)

df4[['month_issue_since_crline', 'month_last_pymnt_since_issue']].head()

df4 = df4.drop(['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d'], axis =1)
df4.head()

"""Selanjutnya akan dilakukan deteksi outlier dan menghapus data apabila terdapat lebih dari 5 kolom yang memiliki outlier."""

def detect_outliers_IQR(df):
    outliers = pd.DataFrame(index=df.index)
    for column in df.columns:
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers[column] = (df[column] < lower_bound) | (df[column] > upper_bound)
    return outliers

df5 = df4.copy()
numerical_cols = df5.select_dtypes(include=[np.number]).columns

outliers = detect_outliers_IQR(df5[numerical_cols])

# Menambahkan kolom yang menghitung jumlah kategori yang memiliki outlier untuk setiap baris
df5['Outlier_Count'] = outliers.sum(axis=1)

# Menghapus baris yang memiliki outlier di lebih dari 5 kategori
filtered_df = df5[df5['Outlier_Count'] <= 5].drop(columns=['Outlier_Count'])

df5.info()
filtered_df.info()

data =filtered_df.copy()

"""<a name="5"></a>
## 5 - Splitting Data
- Agar mempermudah analisis, train.csv dibagi menjadi 2 bagian, yaitu data train dan data tes.
"""

from sklearn.model_selection import train_test_split
X = data.drop('loan_status', axis =1)
y = data['loan_status']

X_train, X_test, y_train, y_test = (train_test_split(X,y,test_size=0.2, stratify=y, random_state=0))

y_train.value_counts()

"""<a name="6"></a>

## 6 - Preprocessing Data (2) - Pipeline
- Beberapa tahapan preprocessing data menggunakan operator `pipeline` pada scikit-learn agar lebih mudah digunakan dan menghindari kemungkinan terjadinya kebocoran (leakage) data tes saat pemodelan.
"""

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, SimpleImputer
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder
from sklearn.preprocessing import MinMaxScaler

class NumImputer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        numeric = X.select_dtypes(include=['int64', 'float64']).columns
        X[numeric] = IterativeImputer().fit_transform(X[numeric])
        return X

class CatImputer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        cat = X.select_dtypes(include=['object']).columns
        X[cat] = SimpleImputer(strategy = 'most_frequent').fit_transform(X[cat])
        return X

class scaler(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        num = X.select_dtypes(include=['int64', 'float64']).columns
        X[num] = MinMaxScaler().fit_transform(X[num])
        return X

class OrdinalEnc(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        oe = OrdinalEncoder(categories=[['< 1 year', '1 year', '2 years', '3 years', '4 years', '5 years', '6 years', '7 years', '8 years', '9 years', '10+ years']])
        X['emp_length'] = oe.fit_transform(X[['emp_length']])
        return X

class Labelenc(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        categorical_cols = X.select_dtypes(include=['object', 'category']).columns

        # Inisialisasi LabelEncoder
        label_encoders = {}

        # Lakukan label encoding untuk setiap kolom kategorik
        for col in categorical_cols:
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col])
            label_encoders[col] = le
        return X

from sklearn.pipeline import Pipeline

pipe_prepo = Pipeline([
    ('num_imputer', NumImputer()),
    ('cat_imputer', CatImputer()),
    ('scaler', scaler()),
    ('ordinal encoder', OrdinalEnc()),
    ('label encoder', Labelenc())
])

X_train.info()

X_train = pipe_prepo.fit_transform(X_train)

X_test = pipe_prepo.transform(X_test)

display(X_train.head(), X_test.head())

X_train.isna().sum().sum()
X_test.isna().sum().sum()

"""<a name="7"></a>
## 7 - Pemodelan
- akan digunakan beberapa model klasifikasi dimana akan memasukkan model Logistic regression juga

**Metrik evaluasi** yang digunakan dan alasannya:
1. **Balance accuracy**, proporsi kelas dengan nilai `FLAG` 1 dan 0 sangatlah tidak seimbang. Dikarenakan ketika data splitting tidak dilakukan penanganan imbalance data. Oleh karena itu, metrik klasifikasi akurasi kurang cocok dan menyesatkan dikarenakan perhitungan akurasi dapat mencapai nilai yang tinggi hanya dengan memprediksi kelas mayoritas secara konsisten. Balance accuracy melakukan perhitungan akurasi dengan mempertimbangkan kelas yang imbalance. Metrik ini juga memberikan gambaran yang lebih realistis dikarenakan melakukan perhitungan untuk kedua kelas dengan cara yang lebih seimbang.
2. **F1-score**, metrik ini cocok untuk data yang imbalance dikarenakan f1-score memperhitungkan nilai precision dan recall sehingga memberikan gambaran yang lebih baik mengenai performa model pada kelas minoritas. F1-score ini juga menunjukkan keseimbangan antara precision dan recall dimana nilai keduanya bisa saja bervariasi secara signifikan.
3. **ROC AUC**, metrik ini merupakan metrik yang umum digunakan untuk data imbalance. Hal ini dikarenakan ROC AUC tidak sensitif terhadap perubahan proporsi kelas. Selain itu, metrik ini juga mengatasi imbalance class karena mempertimbangkan semua tingkat sensitivitas dan spesifisitas.
"""

# pip install catboost

import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score, roc_auc_score, balanced_accuracy_score
# Daftar model yang akan digunakan
list_model = [
    LogisticRegression(random_state=0),
    LogisticRegressionCV(random_state=0),
    GradientBoostingClassifier(random_state=0),
    HistGradientBoostingClassifier(random_state=0),
    CatBoostClassifier(random_state=0),
    XGBClassifier(random_state=0),
    BernoulliNB(),
    GaussianNB(),
    MultinomialNB(),
    KNeighborsClassifier()
]

# List kosong untuk menyimpan semua hasil pengukuran
result = []

for model in list_model:
    # Buat pipeline dengan scaler dan model
    pipeline = Pipeline([
        ('classifier', model)
    ])

    # Fit model
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)

    # Hitung nilai berdasarkan model yang sudah di train / fit dan hasil prediksinya
    bal_accuracy = balanced_accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='binary')
    roc_auc = roc_auc_score(y_test, pipeline.predict_proba(X_test)[:, 1])

    # Kumpulkan nilai-nilai di atas menjadi satu
    hasil = {
        'Model': type(model).__name__,
        'Balanced Accuracy': bal_accuracy,
        'F1 Score': f1,
        'ROC AUC': roc_auc
    }

    # Simpan nilai model ini dalam list result
    result.append(hasil)

# Ubah menjadi dataframe
result_all = pd.DataFrame(result)
print(result_all)

result_all

"""### Model Terbaik

Dipilih model terbaik yaitu klasifikasi dengan metode XGBClassifier(random_state=0) karena menghasilkan Balanced Accuracy, F1-score, dan ROC AUC yang tertinggi dibandingkan seluruh model yang lain.
"""

best = Pipeline([
        ('classifier', XGBClassifier(random_state=0))
    ])
best.fit(X_train, y_train) # fitting model terbaik menggunakan data latih

"""<a name="8"></a>
## 8 - Evaluation

"""

# Prediksi
Y_pred = best.predict(X_test)

# Cek report
from sklearn.metrics import classification_report
print(classification_report(y_test, Y_pred))

# add other
bal_accuracy = balanced_accuracy_score(y_test, Y_pred)
f1 = f1_score(y_test, Y_pred, average='binary')
roc_auc = roc_auc_score(y_test, best.predict_proba(X_test)[:, 1])

# menampilkan score metrik
print(f'Balance Aquracy = {bal_accuracy}\nF1 score \t= {f1}\nROC AUC \t= {roc_auc}')

"""## 9 - Conclusion

Beberapa langkah dalam peningkatan performa model klasifikasi dapat dilakukan dengan cara berikut:

- Melakukan penanganan data imbalance dalam data splitting
- Melakukan cross validation untuk menenjukan hyperparameter
- Melakukan imputasi missing value dengan algoritma machine learning terutama untuk variabel kategorik.

Dengan model ini (atau model yang sudah ditingkatkan) Lending Club dapat menggunakannya untuk **menekan** **risiko** **kerugian** **akibat** **ketidaktepatan** **pemberian** **pinjaman** **kepada customer**. Model yang telah dikembangkan menunjukkan **performa yang luar** **biasa** menggunakan algoritma XG Boost Classifier. Dengan mengimplementasikan ini, Lending Club dapat mengurangi kemungkinan memberikan pinjaman ke calon peminjam yang dirasa tidak akan membayar pinjaman secara penuh. Dampaknya, Lending Club akan dapat secara tepat sasaran memberikan pinjaman ke calon peminjam yang layak dengan karakteristik tertentu.
"""